{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaded31b",
   "metadata": {},
   "source": [
    "# Machine Learning Approaches to Measuring the Impact of Mergers and Acquisitions on Stock Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479f40f",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "### Overview\n",
    "\n",
    "The impact of mergers and acquisitions (M&A) on the stock returns of both proponent (acquiring) and target (selling) firms has been extensively studied. However, most research has been limited to specific sectors or regions[^1].\n",
    "\n",
    "In this project, we aim to leverage **unsupervised machine learning models** to cluster M&A transactions based on their key determinants and fundamental characteristics. These clusters can then be used to identify potential **short-term trading opportunities**.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "We will use the **CRISP-DM**[^2] framework, originally published in 1999, which remains the *de facto* standard for data mining[^3] and is widely recognized for its strong alignment with business objectives.\n",
    "\n",
    "The CRISP-DM process consists of six main steps:\n",
    "\n",
    "1. **Business Understanding** – Define the project objectives and requirements from a business perspective.  \n",
    "2. **Data Understanding** – Collect initial data and become familiar with its structure and quality.  \n",
    "3. **Data Preparation** – Build the final dataset ready for modeling through cleaning, selection, and transformation.  \n",
    "4. **Modeling** – Apply machine learning or statistical models and tune their parameters.  \n",
    "5. **Evaluation** – Ensure that the model meets both technical and business objectives.  \n",
    "6. **Deployment** – Deliver the final model or insights into production or decision-making environments.\n",
    "\n",
    "Please note that **CRISP-DM is an iterative process** — insights from one phase may lead to revisiting and refining earlier phases.\n",
    "\n",
    "In this project, we will focus on the **first five steps**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f1ed8a",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "[ˆ1]: Campa, J. M., & Hernando, I. (2006). M&As performance in the European financial industry. Journal of Banking & Finance.\n",
    "[ˆ2]: Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., & Wirth, R. (2000). CRISP-DM 1.0: Step-by-step data mining guide. CRISP-DM Consortium.\n",
    "[ˆ3]: Schröer, C., Kruse, F., & Gómez, J. M. (2021). A systematic literature review on applying CRISP-DM process model. Procedia Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809abd2f",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "\n",
    "### 1.1. Overview\n",
    "\n",
    "The objective of this project is to classify M&A transactions based on their determinants and fundamental characteristics. This classification aims to help investors identify potential trading opportunities involving the target firms, acquiring firms, or their comparable companies.\n",
    "\n",
    "To align with academic requirements, we will limit the analysis to companies incorporated in the Euro area (Eurozone). If this subset still contains more than 500 transactions, we will randomly select 500 instances for the study.\n",
    "\n",
    "After completing the Data Understanding step, the following information should be collected for each transaction:\n",
    "\n",
    "- **Business description** of the target and the acquiring company, obtained from databases or the internet.  \n",
    "- **Transaction description**, including the rationale of the deal, based on filings, analyses, or news sources.  \n",
    "- **Financial data** for both the target and the acquirer.  \n",
    "- **Financial metrics** specific to the transaction itself.\n",
    "\n",
    "> **Note:** All information should be consistent with the transaction announcement date to prevent data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Business Description\n",
    "\n",
    "In addition to administrative data, such as business name or year of incorporation, a business description should include the following items:\n",
    "\n",
    "- **Mission statement**  \n",
    "- **Products and services**  \n",
    "- **Targeted markets**  \n",
    "- **Targeted customers**  \n",
    "- **Primary technologies** (if applicable)  \n",
    "- **Competitive advantages**  \n",
    "- **Business objectives**\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Transaction Description\n",
    "\n",
    "Since the **Transaction Comments** field already summarizes financial details, the description for each M&A transaction should focus on the **rationale** behind the deal. Research on M&A suggests that the rationale may include one or more of the following categories:\n",
    "\n",
    "- **Operational** – economies of scale, scope, or efficiency gains.  \n",
    "- **Financial** – diversification, access to capital markets, lowering financial costs, deleveraging, or leveraging.  \n",
    "- **Regulatory** – obtaining licenses, patents, or meeting regulatory requirements.  \n",
    "- **Restructuring** – restructuring shareholding structure, voting rights, legal structure.  \n",
    "- **Technology** – acquiring R&D pipelines, innovation capabilities, or fostering an innovative culture.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 Financial Data\n",
    "\n",
    "For each M&A transaction, the following financial data should be collected for both the target and acquiring companies:\n",
    "\n",
    "- **Balance Sheet**: total assets, total liabilities, shareholders’ equity, cash and cash equivalents, short-term and long-term debt.  \n",
    "- **Income Statement**: revenue/sales, cost of goods sold (COGS), operating income (EBIT), net income.  \n",
    "- **Profitability Metrics**: gross margin, operating margin, net margin, return on assets (ROA), return on equity (ROE).  \n",
    "- **Liquidity and Leverage Metrics**: current ratio, quick ratio, debt-to-equity ratio, debt-to-assets ratio.  \n",
    "- **Market Capitalization** at the announcement date.\n",
    "\n",
    "> These data can be collected through APIs such as Yahoo Finance or other financial databases.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 Financial Metrics of Transactions\n",
    "\n",
    "- **Transaction Value** (in USD millions, using historical exchange rates if applicable) — already available in the dataset.  \n",
    "- **Deal Premium** — difference between offer price and pre-announcement market price, if available.  \n",
    "- **Valuation Ratios** — Price-to-Earnings (PER), Price-to-EBITDA, Price-to-Revenue, Price-to-Book Value.\n",
    "\n",
    "> **Note:** All financial data should correspond to the period prior to or at the announcement date to prevent data leakage. Features should be normalized or scaled for clustering to account for differences in company size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f69f1e",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "5 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ca831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze transaction comments (text data)\n",
    "print(\"Text Data Analysis:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'Transaction Comments' in df.columns:\n",
    "    # Text length analysis\n",
    "    df['comment_length'] = df['Transaction Comments'].fillna('').astype(str).apply(len)\n",
    "    df['comment_word_count'] = df['Transaction Comments'].fillna('').astype(str).apply(lambda x: len(x.split()))\n",
    "    \n",
    "    print(f\"\\nComment Length Statistics:\")\n",
    "    print(f\"  Average characters: {df['comment_length'].mean():.0f}\")\n",
    "    print(f\"  Average words: {df['comment_word_count'].mean():.0f}\")\n",
    "    print(f\"  Max length: {df['comment_length'].max():.0f} characters\")\n",
    "    \n",
    "    # Sample a few transaction comments\n",
    "    print(f\"\\n\\nSample Transaction Descriptions:\")\n",
    "    print(\"=\"*70)\n",
    "    for idx, row in df.head(3).iterrows():\n",
    "        print(f\"\\nTransaction #{idx + 1}:\")\n",
    "        print(f\"Target: {row['Target/Issuer']}\")\n",
    "        print(f\"Buyer: {row['Buyers/Investors']}\")\n",
    "        comment = row['Transaction Comments']\n",
    "        if pd.notna(comment):\n",
    "            print(f\"Description: {str(comment)[:300]}...\")\n",
    "        print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb18899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "print(\"Categorical Feature Analysis:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Country distribution\n",
    "if 'Country/Region of Incorporation [Target/Issuer]' in df.columns:\n",
    "    print(\"\\nTop 10 Target Countries:\")\n",
    "    country_counts = df['Country/Region of Incorporation [Target/Issuer]'].value_counts().head(10)\n",
    "    print(country_counts)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    country_counts.plot(kind='barh', color='skyblue')\n",
    "    plt.xlabel('Number of Transactions')\n",
    "    plt.title('Top 10 Target Countries in Eurozone M&A')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Transaction status\n",
    "if 'Transaction Status' in df.columns:\n",
    "    print(\"\\n\\nTransaction Status Distribution:\")\n",
    "    status_counts = df['Transaction Status'].value_counts()\n",
    "    print(status_counts)\n",
    "    \n",
    "    # Pie chart\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    status_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Transaction Status Distribution')\n",
    "    plt.ylabel('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9413ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze transaction values\n",
    "print(\"Transaction Value Analysis:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'Total Transaction Value ($USDmm, Historical rate)' in df.columns:\n",
    "    values = df['Total Transaction Value ($USDmm, Historical rate)'].dropna()\n",
    "    \n",
    "    print(f\"\\nDescriptive Statistics:\")\n",
    "    print(f\"  Mean: ${values.mean():.2f}M\")\n",
    "    print(f\"  Median: ${values.median():.2f}M\")\n",
    "    print(f\"  Std Dev: ${values.std():.2f}M\")\n",
    "    print(f\"  Min: ${values.min():.2f}M\")\n",
    "    print(f\"  Max: ${values.max():.2f}M\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(values, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Transaction Value ($M USD)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of Transaction Values')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1].boxplot(values, vert=True)\n",
    "    axes[1].set_ylabel('Transaction Value ($M USD)')\n",
    "    axes[1].set_title('Transaction Value Box Plot')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data types and missing values\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\n\\nMissing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing Count': missing.values,\n",
    "    'Missing %': missing_pct.values\n",
    "})\n",
    "missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6368c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"Sample Transactions:\")\n",
    "print(\"=\"*100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_file = \"MA Transactions Over 50M.xlsx\"\n",
    "\n",
    "# Check if file exists\n",
    "if Path(data_file).exists():\n",
    "    df = pd.read_excel(data_file, header=1)\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Columns: {df.shape[1]}\")\n",
    "    print(f\"  Transactions: {df.shape[0]}\")\n",
    "else:\n",
    "    print(f\"✗ File not found: {data_file}\")\n",
    "    print(\"Please ensure 'MA Transactions Over 50M.xlsx' is in the same directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d65b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b740b",
   "metadata": {},
   "source": [
    "In this step, the goal is to **explore and understand the dataset** to guide preprocessing and modeling. Key activities include:\n",
    "\n",
    "1. **Import and format**: Import the dataset, parse dates, and rename columns.\n",
    "2. **Explore Feature Types:** Identify numeric, categorical, and textual variables, and understand their meaning and relevance.\n",
    "3. **Statistical Summaries:** Compute basic statistics for numeric features (mean, median, variance) and counts/frequencies for categorical variables.\n",
    "4. **Text Analysis:** Count tokens, analyze text lengths, and inspect sample entries to understand language characteristics, which will inform embedding and modeling choices.\n",
    "5. **Initial Quality Checks:** Detect missing values, outliers, or inconsistencies.\n",
    "6. **Insights for Modeling:** Identify which features will be used for clustering or dimensionality reduction, ensuring they are ready for preprocessing in the next step.\n",
    "\n",
    "Column Labels:\n",
    "\n",
    "- **All Transactions Announced Date** – Date of the official transaction announcement.  \n",
    "- **All Transactions Announced Date (Including Bids and Letters of Intent)** – Date of unofficial or preliminary announcement, including bids and letters of intent.  \n",
    "- **Target/Issuer** – Name of the target company.  \n",
    "- **Exchange Ticker** – Exchange ticker of the target/issuer, if listed.  \n",
    "- **Transaction Types** – Type of transaction; should be limited to *Merger/Acquisition*.  \n",
    "- **Transaction Status** – Current status of the transaction.  \n",
    "- **Total Transaction Value ($USDmm, Historical rate)** – Value of the transaction in USD millions, based on historical exchange rates.  \n",
    "- **Buyers/Investors** – Name of the acquiring company.  \n",
    "- **Sellers** – Name of the sellers or shareholders of the target.  \n",
    "- **CIQ Transaction ID** – Internal Capital IQ identifier.  \n",
    "- **Transaction Comments** – Main financial considerations or notes related to the transaction.  \n",
    "- **Country/Region of Incorporation [Target/Issuer]** – Country where the target company is incorporated.  \n",
    "- **Country/Region of Incorporation [Sellers]** – Country where the sellers are incorporated.  \n",
    "- **Target Security Types** – Type of the security of the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f190b88",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d24f65",
   "metadata": {},
   "source": [
    "In this step, the goal is to **clean, transform, collect, and structure the data** to make it suitable for modeling. Key activities include:\n",
    "\n",
    "1. **Handling Missing Values:** Impute or remove missing data to ensure consistency across features.\n",
    "2. **Encoding Categorical Variables:** Convert categorical data into numeric representations (one-hot encoding, ordinal encoding, or embeddings).\n",
    "3. **Scaling Numeric Features:** Normalize or standardize numeric variables to make them comparable for clustering or distance-based methods.\n",
    "4. **Text Processing:** Clean textual data (remove punctuation, lowercase, etc.) and convert into embeddings (e.g., SBERT) for use in downstream analysis.\n",
    "5. **Feature Engineering:** Create new features, combine or transform existing ones, and reduce dimensionality if needed (PCA, UMAP).\n",
    "6. **Final Dataset Assembly:** Concatenate numeric, categorical, and textual features into a single matrix ready for unsupervised learning algorithms.\n",
    "\n",
    "> **Note:** This step ensures that all features are in a consistent format and scale, allowing clustering or other unsupervised methods to effectively detect patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9945800",
   "metadata": {},
   "source": [
    "To reduce the workload, you can limit the analysis to a sample of 300 transactions with the target in the Eurozone.\n",
    "\n",
    "In this particular project, you will have to collect additional information:\n",
    "* Collect a business description for each target and acquirer.\n",
    "* Extract information from the Transaction Comments.\n",
    "* Build a transaction description from news and analysis.\n",
    "* Find financial data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing pipeline\n",
    "print(\"Running preprocessing pipeline...\")\n",
    "print(\"=\"*70)\n",
    "print(\"This step will:\")\n",
    "print(\"  1. Extract keywords from transaction descriptions\")\n",
    "print(\"  2. Handle missing values\")\n",
    "print(\"  3. Scale numeric features\")\n",
    "print(\"  4. Encode categorical features\")\n",
    "print(\"  5. Generate text embeddings using Sentence-BERT\")\n",
    "print(\"\\nThis may take a few minutes...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if preprocessing already done\n",
    "if Path(\"../X_final.npy\").exists():\n",
    "    print(\"\\n✓ Preprocessed data already exists!\")\n",
    "    X_final = np.load(\"../X_final.npy\")\n",
    "    print(f\"  Feature matrix shape: {X_final.shape}\")\n",
    "    print(f\"  Total features: {X_final.shape[1]}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Run preprocessing.py first:\")\n",
    "    print(\"  Command: python ../preprocessing.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f8a76",
   "metadata": {},
   "source": [
    "### 3.2 Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze deal rationale distribution\n",
    "if Path(enriched_file).exists():\n",
    "    rationale_cols = [c for c in df_enriched.columns if c.startswith('rationale_')]\n",
    "    \n",
    "    if rationale_cols:\n",
    "        print(\"Deal Rationale Analysis:\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        rationale_data = df_enriched[rationale_cols].sum().sort_values(ascending=False)\n",
    "        \n",
    "        # Clean names for display\n",
    "        rationale_data.index = [c.replace('rationale_', '').replace('_', ' ').title() \n",
    "                               for c in rationale_data.index]\n",
    "        \n",
    "        print(\"\\nTotal mentions by category:\")\n",
    "        print(rationale_data)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        rationale_data.plot(kind='bar', color='coral')\n",
    "        plt.xlabel('Deal Rationale Category')\n",
    "        plt.ylabel('Total Mentions Across All Deals')\n",
    "        plt.title('Distribution of M&A Deal Rationales')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975932aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enriched data if available, otherwise run enrichment\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "enriched_file = \"../eurozone_transactions_enriched.csv\"\n",
    "\n",
    "if Path(enriched_file).exists():\n",
    "    df_enriched = pd.read_csv(enriched_file)\n",
    "    print(f\"✓ Loaded enriched dataset: {df_enriched.shape}\")\n",
    "else:\n",
    "    print(\"✗ Enriched data not found. Please run data_loader.py first.\")\n",
    "    print(\"  Command: cd .. && python data_loader.py\")\n",
    "    \n",
    "# Display new columns created during enrichment\n",
    "if Path(enriched_file).exists():\n",
    "    new_cols = [c for c in df_enriched.columns if c not in df.columns]\n",
    "    print(f\"\\nNew enrichment features ({len(new_cols)}):\")\n",
    "    for col in new_cols[:15]:\n",
    "        print(f\"  - {col}\")\n",
    "    if len(new_cols) > 15:\n",
    "        print(f\"  ... and {len(new_cols) - 15} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c700d",
   "metadata": {},
   "source": [
    "### 3.1 Data Enrichment with Business Insights\n",
    "\n",
    "We'll extract deal rationale, financial metrics, and key entities from transaction descriptions to enrich our feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72737b",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5002366e",
   "metadata": {},
   "source": [
    "After dimensionality reduction and clustering, you can optionally conduct an analysis on the cumulative abnormal return (CAR) of your cluster. Here is a link to a detailed computation of the [CAR](https://eventstudy.de/blog/cumulative-abnormal-return/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6cc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with K=3 (recommended based on business analysis)\n",
    "if 'X_pca' in locals():\n",
    "    K = 3  # Can be changed to 4 or other values\n",
    "    print(f\"Applying K-Means clustering with K={K}...\")\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_pca)\n",
    "    \n",
    "    print(f\"✓ Clustering complete!\")\n",
    "    print(f\"\\nCluster distribution:\")\n",
    "    unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        print(f\"  Cluster {cluster_id}: {count} transactions ({count/len(cluster_labels)*100:.1f}%)\")\n",
    "    \n",
    "    # t-SNE for 2D visualization\n",
    "    print(f\"\\nGenerating t-SNE visualization...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    X_tsne = tsne.fit_transform(X_pca)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                         c=cluster_labels, cmap='viridis', \n",
    "                         s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.xlabel('t-SNE Dimension 1')\n",
    "    plt.ylabel('t-SNE Dimension 2')\n",
    "    plt.title(f'M&A Transaction Clusters (K={K}) - t-SNE Visualization')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Visualization complete!\")\n",
    "else:\n",
    "    print(\"✗ PCA data not available. Run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72571e04",
   "metadata": {},
   "source": [
    "### 4.3 Apply Clustering and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382302de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of clusters\n",
    "if 'X_pca' in locals():\n",
    "    print(\"Finding optimal K using Silhouette Score and Elbow Method...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    K_range = range(2, 11)\n",
    "    silhouette_scores = []\n",
    "    inertias = []\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X_pca)\n",
    "        \n",
    "        score = silhouette_score(X_pca, labels)\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        silhouette_scores.append(score)\n",
    "        inertias.append(inertia)\n",
    "        \n",
    "        print(f\"K={k}: Silhouette={score:.4f}, Inertia={inertia:.2f}\")\n",
    "    \n",
    "    # Visualize metrics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Silhouette scores\n",
    "    axes[0].plot(K_range, silhouette_scores, marker='o', linewidth=2, markersize=8)\n",
    "    axes[0].set_xlabel('Number of Clusters (K)')\n",
    "    axes[0].set_ylabel('Silhouette Score')\n",
    "    axes[0].set_title('Silhouette Score vs. K')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    best_k = K_range[np.argmax(silhouette_scores)]\n",
    "    axes[0].axvline(x=best_k, color='r', linestyle='--', \n",
    "                   label=f'Best K={best_k}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Elbow plot\n",
    "    axes[1].plot(K_range, inertias, marker='o', linewidth=2, markersize=8)\n",
    "    axes[1].set_xlabel('Number of Clusters (K)')\n",
    "    axes[1].set_ylabel('Inertia (Within-cluster sum of squares)')\n",
    "    axes[1].set_title('Elbow Method')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Optimal K based on Silhouette Score: {best_k}\")\n",
    "else:\n",
    "    print(\"✗ PCA data not available. Run previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04937207",
   "metadata": {},
   "source": [
    "### 4.2 K-Means Clustering: Finding Optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e51bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data and apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "if Path(\"../X_final.npy\").exists():\n",
    "    X = np.load(\"../X_final.npy\")\n",
    "    print(f\"Loaded feature matrix: {X.shape}\")\n",
    "    \n",
    "    # Apply PCA\n",
    "    print(\"\\nApplying PCA (preserving 95% variance)...\")\n",
    "    pca = PCA(n_components=0.95, random_state=42)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    print(f\"✓ PCA complete!\")\n",
    "    print(f\"  Original dimensions: {X.shape[1]}\")\n",
    "    print(f\"  Reduced dimensions: {X_pca.shape[1]}\")\n",
    "    print(f\"  Variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    \n",
    "    # Plot variance explained\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.plot(cumsum, linewidth=2)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA: Cumulative Variance Explained')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✗ Preprocessed data not found. Run preprocessing.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5b84d",
   "metadata": {},
   "source": [
    "### 4.1 Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476d281",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65960664",
   "metadata": {},
   "source": [
    "### 5.3 Key Findings and Business Implications\n",
    "\n",
    "**Summary of Results:**\n",
    "\n",
    "Based on the clustering analysis, we can identify distinct patterns in M&A transactions:\n",
    "\n",
    "1. **Cluster Differentiation**: Each cluster represents transactions with similar characteristics in terms of deal size, rationale, geographic focus, and sector concentration.\n",
    "\n",
    "2. **Deal Rationale Patterns**: Different clusters show dominant motivations (operational synergies, technology acquisition, market expansion, etc.)\n",
    "\n",
    "3. **Company Relationships**: Identify serial acquirers and potential strategic partnerships within clusters.\n",
    "\n",
    "4. **Trading Opportunities**: Clusters can help identify similar companies and potential takeover targets based on historical patterns.\n",
    "\n",
    "**Next Steps:**\n",
    "- Deep dive into specific transactions within interesting clusters\n",
    "- Analyze stock performance patterns within clusters\n",
    "- Identify emerging M&A trends by cluster\n",
    "- Use clusters for predictive modeling of future deals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive cluster comparison visualizations\n",
    "if 'df_enriched_clustered' in locals():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Deal value comparison\n",
    "    if 'Total Transaction Value ($USDmm, Historical rate)' in df_enriched_clustered.columns:\n",
    "        avg_values = df_enriched_clustered.groupby('Cluster')['Total Transaction Value ($USDmm, Historical rate)'].mean()\n",
    "        avg_values_orig = np.expm1(avg_values)\n",
    "        axes[0, 0].bar(avg_values_orig.index, avg_values_orig.values, color='steelblue')\n",
    "        axes[0, 0].set_xlabel('Cluster')\n",
    "        axes[0, 0].set_ylabel('Average Deal Value ($M USD)')\n",
    "        axes[0, 0].set_title('Average Transaction Value by Cluster')\n",
    "        axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 2. Deal rationale heatmap\n",
    "    rationale_cols = [c for c in df_enriched_clustered.columns if c.startswith('rationale_')]\n",
    "    if rationale_cols:\n",
    "        rationale_means = df_enriched_clustered.groupby('Cluster')[rationale_cols].mean()\n",
    "        rationale_means.columns = [c.replace('rationale_', '').replace('_', ' ').title() \n",
    "                                   for c in rationale_means.columns]\n",
    "        im = axes[0, 1].imshow(rationale_means.T, cmap='YlOrRd', aspect='auto')\n",
    "        axes[0, 1].set_xticks(range(len(rationale_means)))\n",
    "        axes[0, 1].set_xticklabels(rationale_means.index)\n",
    "        axes[0, 1].set_yticks(range(len(rationale_means.columns)))\n",
    "        axes[0, 1].set_yticklabels(rationale_means.columns)\n",
    "        axes[0, 1].set_xlabel('Cluster')\n",
    "        axes[0, 1].set_title('Deal Rationale Intensity Heatmap')\n",
    "        plt.colorbar(im, ax=axes[0, 1])\n",
    "    \n",
    "    # 3. Geographic distribution\n",
    "    if 'Country/Region of Incorporation [Target/Issuer]' in df_enriched_clustered.columns:\n",
    "        top_countries = df_enriched_clustered['Country/Region of Incorporation [Target/Issuer]'].value_counts().head(8)\n",
    "        axes[1, 0].barh(range(len(top_countries)), top_countries.values, color='coral')\n",
    "        axes[1, 0].set_yticks(range(len(top_countries)))\n",
    "        axes[1, 0].set_yticklabels(top_countries.index)\n",
    "        axes[1, 0].set_xlabel('Number of Transactions')\n",
    "        axes[1, 0].set_title('Top 8 Target Countries (All Clusters)')\n",
    "        axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "        axes[1, 0].invert_yaxis()\n",
    "    \n",
    "    # 4. Cluster size pie chart\n",
    "    cluster_sizes = df_enriched_clustered['Cluster'].value_counts().sort_index()\n",
    "    axes[1, 1].pie(cluster_sizes, labels=[f'Cluster {i}' for i in cluster_sizes.index],\n",
    "                   autopct='%1.1f%%', startangle=90, colors=plt.cm.viridis(np.linspace(0, 1, len(cluster_sizes))))\n",
    "    axes[1, 1].set_title('Cluster Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Cluster comparison visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84490e18",
   "metadata": {},
   "source": [
    "### 5.2 Cluster Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "if 'df_enriched_clustered' in locals():\n",
    "    print(\"CLUSTER BUSINESS ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for cluster_id in sorted(df_enriched_clustered['Cluster'].unique()):\n",
    "        cluster_data = df_enriched_clustered[df_enriched_clustered['Cluster'] == cluster_id]\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# CLUSTER {cluster_id}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        print(f\"Size: {len(cluster_data)} transactions ({len(cluster_data)/len(df_enriched_clustered)*100:.1f}%)\")\n",
    "        \n",
    "        # Average deal value\n",
    "        if 'Total Transaction Value ($USDmm, Historical rate)' in cluster_data.columns:\n",
    "            avg_value = np.expm1(cluster_data['Total Transaction Value ($USDmm, Historical rate)'].mean())\n",
    "            print(f\"Average Deal Value: ${avg_value:.2f}M USD\")\n",
    "        \n",
    "        # Dominant deal rationale\n",
    "        rationale_cols = [c for c in cluster_data.columns if c.startswith('rationale_')]\n",
    "        if rationale_cols:\n",
    "            rationale_sums = cluster_data[rationale_cols].sum()\n",
    "            dominant = rationale_sums.idxmax().replace('rationale_', '').replace('_', ' ').title()\n",
    "            print(f\"Dominant Rationale: {dominant}\")\n",
    "        \n",
    "        # Top countries\n",
    "        if 'Country/Region of Incorporation [Target/Issuer]' in cluster_data.columns:\n",
    "            print(f\"\\nTop 3 Countries:\")\n",
    "            for country, count in cluster_data['Country/Region of Incorporation [Target/Issuer]'].value_counts().head(3).items():\n",
    "                pct = (count / len(cluster_data)) * 100\n",
    "                print(f\"  → {country}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Top sectors\n",
    "        if 'Sector' in cluster_data.columns:\n",
    "            print(f\"\\nTop 3 Sectors:\")\n",
    "            for sector, count in cluster_data['Sector'].value_counts().head(3).items():\n",
    "                if pd.notna(sector):\n",
    "                    pct = (count / len(cluster_data)) * 100\n",
    "                    print(f\"  → {sector}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Active acquirers\n",
    "        if 'Buyers/Investors' in cluster_data.columns:\n",
    "            buyers = cluster_data['Buyers/Investors'].value_counts()\n",
    "            active_buyers = buyers[buyers > 1]\n",
    "            if len(active_buyers) > 0:\n",
    "                print(f\"\\nActive Acquirers (multiple deals):\")\n",
    "                for buyer, count in active_buyers.head(3).items():\n",
    "                    print(f\"  → {buyer}: {count} acquisitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clustered data and perform detailed analysis\n",
    "clustered_file = \"../eurozone_transactions_clustered_k3.csv\"\n",
    "\n",
    "if Path(clustered_file).exists():\n",
    "    df_clustered = pd.read_csv(clustered_file)\n",
    "    print(f\"✓ Loaded clustered data: {df_clustered.shape}\")\n",
    "    \n",
    "    # Add cluster labels from our analysis\n",
    "    if 'cluster_labels' in locals():\n",
    "        df_enriched_clustered = df_enriched.copy()\n",
    "        df_enriched_clustered['Cluster'] = cluster_labels\n",
    "    else:\n",
    "        df_enriched_clustered = df_clustered\n",
    "    \n",
    "    print(f\"\\nCluster sizes:\")\n",
    "    print(df_enriched_clustered['Cluster'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"✗ Clustered data not found. Run modeling.py first.\")\n",
    "    print(\"  Command: python ../modeling.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646e79e",
   "metadata": {},
   "source": [
    "### 5.1 Cluster Business Insights\n",
    "\n",
    "Now we'll analyze each cluster to understand the business characteristics, deal rationales, and company relationships."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
